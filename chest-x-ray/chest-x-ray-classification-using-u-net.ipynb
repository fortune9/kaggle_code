{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9681947,"sourceType":"datasetVersion","datasetId":5918106}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:00.70233Z","iopub.execute_input":"2025-01-09T09:49:00.702625Z","iopub.status.idle":"2025-01-09T09:49:16.252825Z","shell.execute_reply.started":"2025-01-09T09:49:00.702591Z","shell.execute_reply":"2025-01-09T09:49:16.251987Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_path = '/kaggle/input/chest-x-ray-dataset-4-categories/Chest X_Ray Dataset/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:16.253659Z","iopub.execute_input":"2025-01-09T09:49:16.254041Z","iopub.status.idle":"2025-01-09T09:49:16.257545Z","shell.execute_reply.started":"2025-01-09T09:49:16.254007Z","shell.execute_reply":"2025-01-09T09:49:16.256935Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_paths = []\ncategories = []\n\ncategories_list = ['COVID19', 'NORMAL', 'PNEUMONIA', 'TURBERCULOSIS']  \n\nfor category in categories_list:\n    category_path = os.path.join(dataset_path, category)\n    \n    if os.path.exists(category_path):\n        \n        for image_name in os.listdir(category_path):\n            image_path = os.path.join(category_path, image_name)\n            image_paths.append(image_path)\n            categories.append(category)\n\ndf = pd.DataFrame({'image_path': image_paths, 'category': categories})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:16.258319Z","iopub.execute_input":"2025-01-09T09:49:16.25862Z","iopub.status.idle":"2025-01-09T09:49:16.299098Z","shell.execute_reply.started":"2025-01-09T09:49:16.258596Z","shell.execute_reply":"2025-01-09T09:49:16.298223Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:16.301271Z","iopub.execute_input":"2025-01-09T09:49:16.301493Z","iopub.status.idle":"2025-01-09T09:49:16.32211Z","shell.execute_reply.started":"2025-01-09T09:49:16.301475Z","shell.execute_reply":"2025-01-09T09:49:16.321319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:16.323432Z","iopub.execute_input":"2025-01-09T09:49:16.323651Z","iopub.status.idle":"2025-01-09T09:49:16.337962Z","shell.execute_reply.started":"2025-01-09T09:49:16.323621Z","shell.execute_reply":"2025-01-09T09:49:16.337111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:16.338649Z","iopub.execute_input":"2025-01-09T09:49:16.338935Z","iopub.status.idle":"2025-01-09T09:49:16.355762Z","shell.execute_reply.started":"2025-01-09T09:49:16.338884Z","shell.execute_reply":"2025-01-09T09:49:16.355081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:16.356468Z","iopub.execute_input":"2025-01-09T09:49:16.356717Z","iopub.status.idle":"2025-01-09T09:49:16.3749Z","shell.execute_reply.started":"2025-01-09T09:49:16.356688Z","shell.execute_reply":"2025-01-09T09:49:16.374051Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:16.375673Z","iopub.execute_input":"2025-01-09T09:49:16.375851Z","iopub.status.idle":"2025-01-09T09:49:16.407929Z","shell.execute_reply.started":"2025-01-09T09:49:16.375835Z","shell.execute_reply":"2025-01-09T09:49:16.407009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:16.408882Z","iopub.execute_input":"2025-01-09T09:49:16.409225Z","iopub.status.idle":"2025-01-09T09:49:16.416021Z","shell.execute_reply.started":"2025-01-09T09:49:16.409192Z","shell.execute_reply":"2025-01-09T09:49:16.415188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:16.417022Z","iopub.execute_input":"2025-01-09T09:49:16.41727Z","iopub.status.idle":"2025-01-09T09:49:16.447169Z","shell.execute_reply.started":"2025-01-09T09:49:16.41725Z","shell.execute_reply":"2025-01-09T09:49:16.446426Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['category'].unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:16.448053Z","iopub.execute_input":"2025-01-09T09:49:16.448391Z","iopub.status.idle":"2025-01-09T09:49:16.467059Z","shell.execute_reply.started":"2025-01-09T09:49:16.448358Z","shell.execute_reply":"2025-01-09T09:49:16.466363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['category'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:16.467748Z","iopub.execute_input":"2025-01-09T09:49:16.467992Z","iopub.status.idle":"2025-01-09T09:49:16.492607Z","shell.execute_reply.started":"2025-01-09T09:49:16.46796Z","shell.execute_reply":"2025-01-09T09:49:16.491892Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ncategory_counts = df['category'].value_counts()\n\nplt.figure(figsize=(8, 6))\nsns.barplot(x=category_counts.index, y=category_counts.values, palette=\"viridis\")\nplt.title(\"Count of Images per Category\")\nplt.xlabel(\"Category\")\nplt.ylabel(\"Number of Images\")\nplt.show()\n\nplt.figure(figsize=(8, 6))\nplt.pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette(\"viridis\", len(category_counts)))\nplt.title(\"Proportion of Images per Category\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:16.496616Z","iopub.execute_input":"2025-01-09T09:49:16.496828Z","iopub.status.idle":"2025-01-09T09:49:17.794963Z","shell.execute_reply.started":"2025-01-09T09:49:16.496808Z","shell.execute_reply":"2025-01-09T09:49:17.793354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nfrom PIL import Image\n\nnum_images_per_category = 5\n\nplt.figure(figsize=(15, 10))\n\nfor i, category in enumerate(df['category'].unique()):\n\n    category_images = df[df['category'] == category]['image_path']\n    \n    selected_images = random.sample(list(category_images), num_images_per_category)\n    \n    for j, image_path in enumerate(selected_images):\n\n        img = Image.open(image_path)\n        \n        plt.subplot(len(df['category'].unique()), num_images_per_category, i * num_images_per_category + j + 1)\n        plt.imshow(img, cmap='gray')\n        plt.axis('off')\n        plt.title(category if j == 0 else \"\") \n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:17.798847Z","iopub.execute_input":"2025-01-09T09:49:17.799588Z","iopub.status.idle":"2025-01-09T09:49:21.361516Z","shell.execute_reply.started":"2025-01-09T09:49:17.799537Z","shell.execute_reply":"2025-01-09T09:49:21.360309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def encoder_block(inputs, num_filters): \n\n\tx = tf.keras.layers.Conv2D(num_filters, \n\t\t\t\t\t\t\t3, \n\t\t\t\t\t\t\tpadding = 'valid')(inputs) \n\tx = tf.keras.layers.Activation('relu')(x) \n\t\n\tx = tf.keras.layers.Conv2D(num_filters, \n\t\t\t\t\t\t\t3, \n\t\t\t\t\t\t\tpadding = 'valid')(x) \n\tx = tf.keras.layers.Activation('relu')(x) \n\n\tx = tf.keras.layers.MaxPool2D(pool_size = (2, 2), \n\t\t\t\t\t\t\t\tstrides = 2)(x) \n\t\n\treturn x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:21.362572Z","iopub.execute_input":"2025-01-09T09:49:21.362836Z","iopub.status.idle":"2025-01-09T09:49:21.367833Z","shell.execute_reply.started":"2025-01-09T09:49:21.362805Z","shell.execute_reply":"2025-01-09T09:49:21.367054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.layers import Lambda\n\ndef decoder_block(inputs, skip_features, num_filters): \n   \n    x = tf.keras.layers.Conv2DTranspose(num_filters, \n                                        (2, 2), \n                                        strides = 2, \n                                        padding = 'valid')(inputs) \n    \n   \n    x = Lambda(lambda x: tf.image.resize(x, size=(skip_features.shape[1], skip_features.shape[2])))(x)\n\n    x = tf.keras.layers.Concatenate()([x, skip_features]) \n    \n    x = tf.keras.layers.Conv2D(num_filters, 3, padding = 'valid')(x) \n    x = tf.keras.layers.Activation('relu')(x) \n\n    x = tf.keras.layers.Conv2D(num_filters, 3, padding = 'valid')(x) \n    x = tf.keras.layers.Activation('relu')(x) \n    \n    return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:21.369061Z","iopub.execute_input":"2025-01-09T09:49:21.369412Z","iopub.status.idle":"2025-01-09T09:49:35.494232Z","shell.execute_reply.started":"2025-01-09T09:49:21.369387Z","shell.execute_reply":"2025-01-09T09:49:35.493315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf \n\ndef unet_model(input_shape = (256, 256, 3), num_classes = 2): \n\tinputs = tf.keras.layers.Input(input_shape) \n\t\n\ts1 = encoder_block(inputs, 64) \n\ts2 = encoder_block(s1, 128) \n\ts3 = encoder_block(s2, 256) \n\ts4 = encoder_block(s3, 512) \n\t\n\tb1 = tf.keras.layers.Conv2D(1024, 3, padding = 'valid')(s4) \n\tb1 = tf.keras.layers.Activation('relu')(b1) \n\tb1 = tf.keras.layers.Conv2D(1024, 3, padding = 'valid')(b1) \n\tb1 = tf.keras.layers.Activation('relu')(b1) \n\t\n\ts5 = decoder_block(b1, s4, 512) \n\ts6 = decoder_block(s5, s3, 256) \n\ts7 = decoder_block(s6, s2, 128) \n\ts8 = decoder_block(s7, s1, 64) \n\t\n\toutputs = tf.keras.layers.Conv2D(num_classes, \n\t\t\t\t\t\t\t\t\t1, \n\t\t\t\t\t\t\t\t\tpadding = 'valid', \n\t\t\t\t\t\t\t\t\tactivation = 'sigmoid')(s8) \n\t\n\tmodel = tf.keras.models.Model(inputs = inputs, \n\t\t\t\t\t\t\t\toutputs = outputs, \n\t\t\t\t\t\t\t\tname = 'U-Net') \n\treturn model \n\nif __name__ == '__main__': \n\tmodel = unet_model(input_shape=(572, 572, 3), num_classes=2) \n\tmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:35.495195Z","iopub.execute_input":"2025-01-09T09:49:35.495654Z","iopub.status.idle":"2025-01-09T09:49:37.163129Z","shell.execute_reply.started":"2025-01-09T09:49:35.495633Z","shell.execute_reply":"2025-01-09T09:49:37.162281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np \nfrom PIL import Image \nfrom tensorflow.keras.preprocessing import image \n \nimg = Image.open('/kaggle/input/chest-x-ray-dataset-4-categories/Chest X_Ray Dataset/COVID19/COVID19(0).jpg')\n\nimg = img.convert('RGB')\n\nimg = img.resize((572, 572)) \nimg_array = image.img_to_array(img) \n\nimg_array = np.expand_dims(img_array, axis=0) \nimg_array = img_array / 255.0\n\nmodel = unet_model(input_shape=(572, 572, 3), num_classes=4)\n\npredictions = model.predict(img_array)\n\npredictions = np.squeeze(predictions, axis=0)\npredictions = np.argmax(predictions, axis=-1)\n\npredictions = Image.fromarray(np.uint8(predictions * 255))\npredictions = predictions.resize((img.width, img.height))\n\npredictions.save('predicted_image.jpg') \npredictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:37.163985Z","iopub.execute_input":"2025-01-09T09:49:37.16433Z","iopub.status.idle":"2025-01-09T09:49:49.355181Z","shell.execute_reply.started":"2025-01-09T09:49:37.164296Z","shell.execute_reply":"2025-01-09T09:49:49.354237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img = Image.open('/kaggle/input/chest-x-ray-dataset-4-categories/Chest X_Ray Dataset/NORMAL/IM-0001-0001.jpeg')\n\nimg = img.convert('RGB')\n\nimg = img.resize((572, 572)) \nimg_array = image.img_to_array(img) \n\nimg_array = np.expand_dims(img_array, axis=0) \nimg_array = img_array / 255.0\n\nmodel = unet_model(input_shape=(572, 572, 3), num_classes=2)\n\npredictions = model.predict(img_array)\n\npredictions = np.squeeze(predictions, axis=0)\npredictions = np.argmax(predictions, axis=-1)\n\npredictions = Image.fromarray(np.uint8(predictions * 255))\npredictions = predictions.resize((img.width, img.height))\n\npredictions.save('predicted_image.jpg') \npredictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:49.356144Z","iopub.execute_input":"2025-01-09T09:49:49.356497Z","iopub.status.idle":"2025-01-09T09:49:51.02702Z","shell.execute_reply.started":"2025-01-09T09:49:49.356463Z","shell.execute_reply":"2025-01-09T09:49:51.025996Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img = Image.open('/kaggle/input/chest-x-ray-dataset-4-categories/Chest X_Ray Dataset/PNEUMONIA/person1000_bacteria_2931.jpeg')\n\nimg = img.convert('RGB')\n\nimg = img.resize((572, 572)) \nimg_array = image.img_to_array(img) \n\nimg_array = np.expand_dims(img_array, axis=0) \nimg_array = img_array / 255.0\n\nmodel = unet_model(input_shape=(572, 572, 3), num_classes=2)\n\npredictions = model.predict(img_array)\n\npredictions = np.squeeze(predictions, axis=0)\npredictions = np.argmax(predictions, axis=-1)\n\npredictions = Image.fromarray(np.uint8(predictions * 255))\npredictions = predictions.resize((img.width, img.height))\n\npredictions.save('predicted_image.jpg') \npredictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:51.027963Z","iopub.execute_input":"2025-01-09T09:49:51.028315Z","iopub.status.idle":"2025-01-09T09:49:52.233957Z","shell.execute_reply.started":"2025-01-09T09:49:51.028283Z","shell.execute_reply":"2025-01-09T09:49:52.232666Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img = Image.open('/kaggle/input/chest-x-ray-dataset-4-categories/Chest X_Ray Dataset/TURBERCULOSIS/Tuberculosis-1.png')\n\nimg = img.convert('RGB')\n\nimg = img.resize((572, 572)) \nimg_array = image.img_to_array(img) \n\nimg_array = np.expand_dims(img_array, axis=0) \nimg_array = img_array / 255.0\n\nmodel = unet_model(input_shape=(572, 572, 3), num_classes=2)\n\npredictions = model.predict(img_array)\n\npredictions = np.squeeze(predictions, axis=0)\npredictions = np.argmax(predictions, axis=-1)\n\npredictions = Image.fromarray(np.uint8(predictions * 255))\npredictions = predictions.resize((img.width, img.height))\n\npredictions.save('predicted_image.jpg') \npredictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:52.235025Z","iopub.execute_input":"2025-01-09T09:49:52.235327Z","iopub.status.idle":"2025-01-09T09:49:53.421688Z","shell.execute_reply.started":"2025-01-09T09:49:52.235299Z","shell.execute_reply":"2025-01-09T09:49:53.420845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\n\ndf['category_encoded'] = label_encoder.fit_transform(df['category'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:53.422605Z","iopub.execute_input":"2025-01-09T09:49:53.422964Z","iopub.status.idle":"2025-01-09T09:49:53.519171Z","shell.execute_reply.started":"2025-01-09T09:49:53.422909Z","shell.execute_reply":"2025-01-09T09:49:53.518147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:53.520467Z","iopub.execute_input":"2025-01-09T09:49:53.520843Z","iopub.status.idle":"2025-01-09T09:49:53.532446Z","shell.execute_reply.started":"2025-01-09T09:49:53.520805Z","shell.execute_reply":"2025-01-09T09:49:53.531654Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df[['image_path', 'category_encoded']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:53.533475Z","iopub.execute_input":"2025-01-09T09:49:53.533791Z","iopub.status.idle":"2025-01-09T09:49:53.552513Z","shell.execute_reply.started":"2025-01-09T09:49:53.533758Z","shell.execute_reply":"2025-01-09T09:49:53.551229Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:53.553567Z","iopub.execute_input":"2025-01-09T09:49:53.553948Z","iopub.status.idle":"2025-01-09T09:49:54.651479Z","shell.execute_reply.started":"2025-01-09T09:49:53.553894Z","shell.execute_reply":"2025-01-09T09:49:54.650461Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ros = RandomOverSampler(random_state=42)\nX_resampled, y_resampled = ros.fit_resample(df[['image_path']], df['category_encoded'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:54.652502Z","iopub.execute_input":"2025-01-09T09:49:54.653315Z","iopub.status.idle":"2025-01-09T09:49:54.668818Z","shell.execute_reply.started":"2025-01-09T09:49:54.653267Z","shell.execute_reply":"2025-01-09T09:49:54.667597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_resampled = pd.DataFrame(X_resampled, columns=['image_path'])\ndf_resampled['category_encoded'] = y_resampled","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:54.670026Z","iopub.execute_input":"2025-01-09T09:49:54.670366Z","iopub.status.idle":"2025-01-09T09:49:54.681286Z","shell.execute_reply.started":"2025-01-09T09:49:54.670339Z","shell.execute_reply":"2025-01-09T09:49:54.68042Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nClass distribution after oversampling:\")\nprint(df_resampled['category_encoded'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:54.682339Z","iopub.execute_input":"2025-01-09T09:49:54.682722Z","iopub.status.idle":"2025-01-09T09:49:54.704156Z","shell.execute_reply.started":"2025-01-09T09:49:54.682694Z","shell.execute_reply":"2025-01-09T09:49:54.702902Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_resampled","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:54.705645Z","iopub.execute_input":"2025-01-09T09:49:54.706038Z","iopub.status.idle":"2025-01-09T09:49:54.727249Z","shell.execute_reply.started":"2025-01-09T09:49:54.706001Z","shell.execute_reply":"2025-01-09T09:49:54.726289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_resampled['category_encoded'] = df_resampled['category_encoded'].astype(str)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:54.728268Z","iopub.execute_input":"2025-01-09T09:49:54.728541Z","iopub.status.idle":"2025-01-09T09:49:54.749102Z","shell.execute_reply.started":"2025-01-09T09:49:54.728519Z","shell.execute_reply":"2025-01-09T09:49:54.748026Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"import os\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\nimport numpy as np\n\n# Define the U-Net model (assuming encoder_block and decoder_block are defined)\ndef unet_model(input_shape=(256, 256, 3), num_classes=2):\n    inputs = tf.keras.layers.Input(input_shape) \n    \n    s1 = encoder_block(inputs, 64) \n    s2 = encoder_block(s1, 128) \n    s3 = encoder_block(s2, 256) \n    s4 = encoder_block(s3, 512) \n    \n    b1 = tf.keras.layers.Conv2D(1024, 3, padding='valid')(s4) \n    b1 = tf.keras.layers.Activation('relu')(b1) \n    b1 = tf.keras.layers.Conv2D(1024, 3, padding='valid')(b1) \n    b1 = tf.keras.layers.Activation('relu')(b1) \n    \n    s5 = decoder_block(b1, s4, 512) \n    s6 = decoder_block(s5, s3, 256) \n    s7 = decoder_block(s6, s2, 128) \n    s8 = decoder_block(s7, s1, 64) \n    \n    outputs = tf.keras.layers.Conv2D(num_classes, 1, padding='valid', activation='sigmoid')(s8) \n    \n    model = tf.keras.models.Model(inputs=inputs, outputs=outputs, name='U-Net') \n    return model \n\n# Function to create folders for each class\ndef create_folders():\n    class_names = ['COVID19', 'NORMAL', 'PNEUMONIA', 'TUBERCULOSIS']\n    for class_name in class_names:\n        class_dir = os.path.join('/kaggle/working/', class_name)\n        if not os.path.exists(class_dir):\n            os.makedirs(class_dir)\n\n# Function to generate segmentation masks\ndef generate_segmentation_masks(df, model, img_size=(256, 256)):\n    # Define the class names based on your categories\n    class_names = ['COVID19', 'NORMAL', 'PNEUMONIA', 'TUBERCULOSIS']\n    \n    # Ensure category_encoded is of integer type\n    df['category_encoded'] = pd.to_numeric(df['category_encoded'], errors='coerce')\n    \n    for idx, row in df.iterrows():\n        image_path = row['image_path']\n        class_label = row['category_encoded']  # This should now be an integer\n        \n        # Ensure the class label is within the correct range\n        if class_label < 0 or class_label >= len(class_names):\n            print(f\"Warning: Invalid class label {class_label} for image {image_path}. Skipping.\")\n            continue\n        \n        # Define the class name based on the label (integer value)\n        class_name = class_names[class_label]  # Map class_label to class name\n        \n        # Load and preprocess the image\n        img = tf.keras.preprocessing.image.load_img(image_path, target_size=img_size)\n        img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n        img_array = tf.expand_dims(img_array, axis=0)  # Add batch dimension\n        \n        # Predict segmentation mask using the trained model\n        pred_mask = model.predict(img_array)\n        pred_mask = tf.squeeze(pred_mask, axis=0)  # Remove batch dimension\n        \n        # If the model has 2 channels (e.g., for binary classification), select one channel or threshold\n        if pred_mask.shape[-1] == 2:\n            # Choose the channel of interest (e.g., channel for the 'COVID19' class)\n            pred_mask = np.argmax(pred_mask, axis=-1)  # Convert to single-channel mask (class prediction)\n        \n        # If it's a binary mask, ensure it's binary (0 or 1)\n        pred_mask = (pred_mask > 0.5).astype(np.uint8)\n        \n        # Add the channel dimension to make it (height, width, 1)\n        pred_mask = np.expand_dims(pred_mask, axis=-1)\n        \n        # Create the directory for the class if it doesn't exist\n        output_dir = f'/kaggle/working/{class_name}'\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # Save the predicted mask as an image\n        mask_image_path = os.path.join(output_dir, f'{os.path.basename(image_path)}_mask.png')\n        tf.keras.preprocessing.image.save_img(mask_image_path, pred_mask)\n        \n        print(f\"Saved mask for {image_path} to {mask_image_path}\")\n\n# Now, you can use the model and df_resampled to generate masks\n# Example usage:\n# Assuming df_resampled is your dataframe\nmodel = unet_model(input_shape=(256, 256, 3), num_classes=2)  # Adjust num_classes for your use case\ngenerate_segmentation_masks(df_resampled, model)\n","metadata":{"execution":{"iopub.status.busy":"2025-01-09T09:11:24.724007Z","iopub.execute_input":"2025-01-09T09:11:24.724238Z","iopub.status.idle":"2025-01-09T09:11:29.743305Z","shell.execute_reply.started":"2025-01-09T09:11:24.724217Z","shell.execute_reply":"2025-01-09T09:11:29.742026Z"}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\n\ndef unet_model(input_shape=(256, 256, 3), num_classes=2):\n    inputs = tf.keras.layers.Input(input_shape) \n    \n    s1 = encoder_block(inputs, 64) \n    s2 = encoder_block(s1, 128) \n    s3 = encoder_block(s2, 256) \n    s4 = encoder_block(s3, 512) \n    \n    b1 = tf.keras.layers.Conv2D(1024, 3, padding='valid')(s4) \n    b1 = tf.keras.layers.Activation('relu')(b1) \n    b1 = tf.keras.layers.Conv2D(1024, 3, padding='valid')(b1) \n    b1 = tf.keras.layers.Activation('relu')(b1) \n    \n    s5 = decoder_block(b1, s4, 512) \n    s6 = decoder_block(s5, s3, 256) \n    s7 = decoder_block(s6, s2, 128) \n    s8 = decoder_block(s7, s1, 64) \n    \n    outputs = tf.keras.layers.Conv2D(num_classes, 1, padding='valid', activation='sigmoid')(s8) \n    \n    model = tf.keras.models.Model(inputs=inputs, outputs=outputs, name='U-Net') \n    return model\n\ndef create_folders():\n    class_names = ['COVID19', 'NORMAL', 'PNEUMONIA', 'TUBERCULOSIS']\n    for class_name in class_names:\n        class_dir = os.path.join('/kaggle/working/', class_name)\n        if not os.path.exists(class_dir):\n            os.makedirs(class_dir)\n\ndef generate_segmentation_masks(df, model, img_size=(256, 256), num_images_per_class=100):\n  \n    class_names = ['COVID19', 'NORMAL', 'PNEUMONIA', 'TUBERCULOSIS']\n    \n    df['category_encoded'] = pd.to_numeric(df['category_encoded'], errors='coerce')\n    \n    for class_label in range(len(class_names)):\n        class_df = df[df['category_encoded'] == class_label].sample(n=num_images_per_class, random_state=42)\n        \n        for idx, row in class_df.iterrows():\n            image_path = row['image_path']\n            class_label = row['category_encoded']  \n          \n            class_name = class_names[class_label]  \n            \n            img = tf.keras.preprocessing.image.load_img(image_path, target_size=img_size)\n            img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n            img_array = tf.expand_dims(img_array, axis=0)  \n            \n            pred_mask = model.predict(img_array)\n            pred_mask = tf.squeeze(pred_mask, axis=0) \n            \n            if pred_mask.shape[-1] == 2:\n                \n                pred_mask = np.argmax(pred_mask, axis=-1)  \n            \n            pred_mask = (pred_mask > 0.5).astype(np.uint8)\n            \n            pred_mask = np.expand_dims(pred_mask, axis=-1)\n            \n            output_dir = f'/kaggle/working/{class_name}'\n            os.makedirs(output_dir, exist_ok=True)\n            \n            mask_image_path = os.path.join(output_dir, f'{os.path.basename(image_path)}_mask.png')\n            tf.keras.preprocessing.image.save_img(mask_image_path, pred_mask)\n            \n            print(f\"Saved mask for {image_path} to {mask_image_path}\")\n\n\nmodel = unet_model(input_shape=(256, 256, 3), num_classes=2) \ngenerate_segmentation_masks(df_resampled, model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:49:54.750238Z","iopub.execute_input":"2025-01-09T09:49:54.750548Z","iopub.status.idle":"2025-01-09T09:50:44.279716Z","shell.execute_reply.started":"2025-01-09T09:49:54.750524Z","shell.execute_reply":"2025-01-09T09:50:44.278858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_resampled","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:50:44.280714Z","iopub.execute_input":"2025-01-09T09:50:44.281054Z","iopub.status.idle":"2025-01-09T09:50:44.2914Z","shell.execute_reply.started":"2025-01-09T09:50:44.281029Z","shell.execute_reply":"2025-01-09T09:50:44.290502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\nfrom tensorflow.keras import regularizers\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint ('check')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:50:44.292297Z","iopub.execute_input":"2025-01-09T09:50:44.29261Z","iopub.status.idle":"2025-01-09T09:50:45.03565Z","shell.execute_reply.started":"2025-01-09T09:50:44.292582Z","shell.execute_reply":"2025-01-09T09:50:45.034693Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_resampled['category_encoded'] = df_resampled['category_encoded'].astype(str)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:50:45.036611Z","iopub.execute_input":"2025-01-09T09:50:45.036877Z","iopub.status.idle":"2025-01-09T09:50:45.04735Z","shell.execute_reply.started":"2025-01-09T09:50:45.036844Z","shell.execute_reply":"2025-01-09T09:50:45.046573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df_new, temp_df_new = train_test_split(\n    df_resampled,\n    train_size=0.8,  \n    shuffle=True,\n    random_state=42,\n    stratify=df_resampled['category_encoded']  \n)\n\nvalid_df_new, test_df_new = train_test_split(\n    temp_df_new,\n    test_size=0.5,  \n    shuffle=True,\n    random_state=42,\n    stratify=temp_df_new['category_encoded'] \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:50:45.048221Z","iopub.execute_input":"2025-01-09T09:50:45.04844Z","iopub.status.idle":"2025-01-09T09:50:45.090934Z","shell.execute_reply.started":"2025-01-09T09:50:45.048423Z","shell.execute_reply":"2025-01-09T09:50:45.089873Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 16\nimg_size = (256, 256)\nchannels = 3  \nimg_shape = (img_size[0], img_size[1], channels)\n\ntr_gen = ImageDataGenerator(rescale=1./255)  \nts_gen = ImageDataGenerator(rescale=1./255)\n\ntrain_gen_new = tr_gen.flow_from_dataframe(\n    train_df_new,\n    x_col='image_path',  \n    y_col='category_encoded',     \n    target_size=img_size,\n    class_mode='sparse',  \n    color_mode='rgb', \n    shuffle=True,\n    batch_size=batch_size\n)\n\nvalid_gen_new = ts_gen.flow_from_dataframe(\n    valid_df_new,\n    x_col='image_path',  \n    y_col='category_encoded',     \n    target_size=img_size,\n    class_mode='sparse',  \n    color_mode='rgb', \n    shuffle=True,\n    batch_size=batch_size\n)\n\ntest_gen_new = ts_gen.flow_from_dataframe(\n    test_df_new,\n    x_col='image_path', \n    y_col='category_encoded',    \n    target_size=img_size,\n    class_mode='sparse',  \n    color_mode='rgb', \n    shuffle=False,  \n    batch_size=batch_size\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:50:45.092081Z","iopub.execute_input":"2025-01-09T09:50:45.092389Z","iopub.status.idle":"2025-01-09T09:50:53.687406Z","shell.execute_reply.started":"2025-01-09T09:50:45.092363Z","shell.execute_reply":"2025-01-09T09:50:53.686512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"physical_devices = tf.config.list_physical_devices('GPU')\nif physical_devices:\n    print(\"Using GPU\")\nelse:\n    print(\"Using CPU\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:50:53.68836Z","iopub.execute_input":"2025-01-09T09:50:53.688676Z","iopub.status.idle":"2025-01-09T09:50:53.693831Z","shell.execute_reply.started":"2025-01-09T09:50:53.688641Z","shell.execute_reply":"2025-01-09T09:50:53.692996Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras import layers, models\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:50:53.694838Z","iopub.execute_input":"2025-01-09T09:50:53.695162Z","iopub.status.idle":"2025-01-09T09:50:53.7183Z","shell.execute_reply.started":"2025-01-09T09:50:53.695127Z","shell.execute_reply":"2025-01-09T09:50:53.7175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:50:53.719192Z","iopub.execute_input":"2025-01-09T09:50:53.719389Z","iopub.status.idle":"2025-01-09T09:50:53.724844Z","shell.execute_reply.started":"2025-01-09T09:50:53.719371Z","shell.execute_reply":"2025-01-09T09:50:53.724272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def unet_classification_model(input_size=(256, 256, 3), num_classes=4):\n    inputs = layers.Input(input_size)\n\n    conv1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n    conv1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)\n    pool1 = layers.MaxPooling2D((2, 2))(conv1)\n\n    conv2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)\n    conv2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv2)\n    pool2 = layers.MaxPooling2D((2, 2))(conv2)\n\n    conv3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)\n    conv3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)\n    pool3 = layers.MaxPooling2D((2, 2))(conv3)\n\n    bottleneck = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(pool3)\n    bottleneck = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(bottleneck)\n\n    upconv3 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(bottleneck)\n    concat3 = layers.concatenate([upconv3, conv3])\n    conv4 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(concat3)\n    conv4 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n\n    upconv2 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv4)\n    concat2 = layers.concatenate([upconv2, conv2])\n    conv5 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(concat2)\n    conv5 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv5)\n\n    upconv1 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv5)\n    concat1 = layers.concatenate([upconv1, conv1])\n    conv6 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(concat1)\n    conv6 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv6)\n\n    gap = layers.GlobalAveragePooling2D()(conv6)  \n\n    output = layers.Dense(num_classes, activation='softmax')(gap)\n\n    model = tf.keras.Model(inputs=inputs, outputs=output)\n    return model\n\nmodel = unet_classification_model(input_size=(256, 256, 3), num_classes=4)\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:50:53.729293Z","iopub.execute_input":"2025-01-09T09:50:53.729516Z","iopub.status.idle":"2025-01-09T09:50:53.902878Z","shell.execute_reply.started":"2025-01-09T09:50:53.729495Z","shell.execute_reply":"2025-01-09T09:50:53.902164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:50:53.904361Z","iopub.execute_input":"2025-01-09T09:50:53.904595Z","iopub.status.idle":"2025-01-09T09:50:53.91886Z","shell.execute_reply.started":"2025-01-09T09:50:53.904574Z","shell.execute_reply":"2025-01-09T09:50:53.917956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_gen_new,  \n    epochs=5,\n    batch_size=16,\n    validation_data=valid_gen_new,  \n    steps_per_epoch=train_gen_new.samples // train_gen_new.batch_size,\n    validation_steps=valid_gen_new.samples // valid_gen_new.batch_size,\n    verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:50:53.920035Z","iopub.execute_input":"2025-01-09T09:50:53.920365Z","iopub.status.idle":"2025-01-09T10:21:14.08182Z","shell.execute_reply.started":"2025-01-09T09:50:53.920336Z","shell.execute_reply":"2025-01-09T10:21:14.081027Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model Accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Model Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:21:14.083011Z","iopub.execute_input":"2025-01-09T10:21:14.083316Z","iopub.status.idle":"2025-01-09T10:21:14.482679Z","shell.execute_reply.started":"2025-01-09T10:21:14.083282Z","shell.execute_reply":"2025-01-09T10:21:14.481726Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_probs = model.predict(test_gen_new)\ny_pred = np.argmax(y_pred_probs, axis=1) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:21:14.483471Z","iopub.execute_input":"2025-01-09T10:21:14.48371Z","iopub.status.idle":"2025-01-09T10:22:05.875609Z","shell.execute_reply.started":"2025-01-09T10:21:14.483689Z","shell.execute_reply":"2025-01-09T10:22:05.874751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_true = test_gen_new.classes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:22:05.876493Z","iopub.execute_input":"2025-01-09T10:22:05.876786Z","iopub.status.idle":"2025-01-09T10:22:05.880384Z","shell.execute_reply.started":"2025-01-09T10:22:05.876751Z","shell.execute_reply":"2025-01-09T10:22:05.879622Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(y_true, y_pred)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_gen_new.class_indices.keys(), yticklabels=test_gen_new.class_indices.keys())\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:22:05.881283Z","iopub.execute_input":"2025-01-09T10:22:05.88158Z","iopub.status.idle":"2025-01-09T10:22:06.129383Z","shell.execute_reply.started":"2025-01-09T10:22:05.881554Z","shell.execute_reply":"2025-01-09T10:22:06.128555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nreport = classification_report(y_true, y_pred, target_names=test_gen_new.class_indices.keys())\nprint(\"Classification Report:\\n\", report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:22:06.13024Z","iopub.execute_input":"2025-01-09T10:22:06.130617Z","iopub.status.idle":"2025-01-09T10:22:06.151153Z","shell.execute_reply.started":"2025-01-09T10:22:06.130581Z","shell.execute_reply":"2025-01-09T10:22:06.150406Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def attention_gate(x, g, inter_channels):\n    \"\"\"Attention Gate Implementation.\"\"\"\n    theta_x = layers.Conv2D(inter_channels, (1, 1), strides=(1, 1), padding='same')(x)\n    phi_g = layers.Conv2D(inter_channels, (1, 1), strides=(1, 1), padding='same')(g)\n    add_xg = layers.Add()([theta_x, phi_g])\n    relu_xg = layers.Activation('relu')(add_xg)\n    psi = layers.Conv2D(1, (1, 1), strides=(1, 1), padding='same')(relu_xg)\n    sigmoid_xg = layers.Activation('sigmoid')(psi)\n    attention = layers.Multiply()([x, sigmoid_xg])\n    return attention\n\ndef unet_with_attention(input_size=(256, 256, 3), num_classes=4):\n    inputs = layers.Input(input_size)\n\n    conv1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n    conv1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)\n    pool1 = layers.MaxPooling2D((2, 2))(conv1)\n\n    conv2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)\n    conv2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv2)\n    pool2 = layers.MaxPooling2D((2, 2))(conv2)\n\n    conv3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)\n    conv3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)\n    pool3 = layers.MaxPooling2D((2, 2))(conv3)\n\n    bottleneck = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(pool3)\n    bottleneck = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(bottleneck)\n\n    upconv3 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(bottleneck)\n    attn3 = attention_gate(conv3, upconv3, inter_channels=128)\n    concat3 = layers.concatenate([upconv3, attn3])\n    conv4 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(concat3)\n    conv4 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n\n    upconv2 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv4)\n    attn2 = attention_gate(conv2, upconv2, inter_channels=64)\n    concat2 = layers.concatenate([upconv2, attn2])\n    conv5 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(concat2)\n    conv5 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv5)\n\n    upconv1 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv5)\n    attn1 = attention_gate(conv1, upconv1, inter_channels=32)\n    concat1 = layers.concatenate([upconv1, attn1])\n    conv6 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(concat1)\n    conv6 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv6)\n\n    gap = layers.GlobalAveragePooling2D()(conv6)  \n    output = layers.Dense(num_classes, activation='softmax')(gap)\n\n    model = tf.keras.Model(inputs=inputs, outputs=output)\n    return model\n\nmodel = unet_with_attention(input_size=(256, 256, 3), num_classes=4)\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:22:06.151871Z","iopub.execute_input":"2025-01-09T10:22:06.152217Z","iopub.status.idle":"2025-01-09T10:22:06.391523Z","shell.execute_reply.started":"2025-01-09T10:22:06.152185Z","shell.execute_reply":"2025-01-09T10:22:06.390823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:22:06.39233Z","iopub.execute_input":"2025-01-09T10:22:06.392646Z","iopub.status.idle":"2025-01-09T10:22:06.40012Z","shell.execute_reply.started":"2025-01-09T10:22:06.392616Z","shell.execute_reply":"2025-01-09T10:22:06.399179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_gen_new,  \n    epochs=5,\n    batch_size=16,\n    validation_data=valid_gen_new,  \n    steps_per_epoch=train_gen_new.samples // train_gen_new.batch_size,\n    validation_steps=valid_gen_new.samples // valid_gen_new.batch_size,\n    verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:22:06.401184Z","iopub.execute_input":"2025-01-09T10:22:06.401499Z","iopub.status.idle":"2025-01-09T10:54:09.602636Z","shell.execute_reply.started":"2025-01-09T10:22:06.401466Z","shell.execute_reply":"2025-01-09T10:54:09.601864Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model Accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Model Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:54:09.603749Z","iopub.execute_input":"2025-01-09T10:54:09.604047Z","iopub.status.idle":"2025-01-09T10:54:10.46445Z","shell.execute_reply.started":"2025-01-09T10:54:09.604021Z","shell.execute_reply":"2025-01-09T10:54:10.463167Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_probs = model.predict(test_gen_new)\ny_pred = np.argmax(y_pred_probs, axis=1) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:54:10.465471Z","iopub.execute_input":"2025-01-09T10:54:10.4657Z","iopub.status.idle":"2025-01-09T10:54:39.402429Z","shell.execute_reply.started":"2025-01-09T10:54:10.46568Z","shell.execute_reply":"2025-01-09T10:54:39.401305Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_true = test_gen_new.classes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:54:39.403641Z","iopub.execute_input":"2025-01-09T10:54:39.404026Z","iopub.status.idle":"2025-01-09T10:54:39.408044Z","shell.execute_reply.started":"2025-01-09T10:54:39.403974Z","shell.execute_reply":"2025-01-09T10:54:39.407248Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(y_true, y_pred)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_gen_new.class_indices.keys(), yticklabels=test_gen_new.class_indices.keys())\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:54:39.408833Z","iopub.execute_input":"2025-01-09T10:54:39.409147Z","iopub.status.idle":"2025-01-09T10:54:39.757362Z","shell.execute_reply.started":"2025-01-09T10:54:39.409124Z","shell.execute_reply":"2025-01-09T10:54:39.756368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"report = classification_report(y_true, y_pred, target_names=test_gen_new.class_indices.keys())\nprint(\"Classification Report:\\n\", report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:54:39.758232Z","iopub.execute_input":"2025-01-09T10:54:39.758493Z","iopub.status.idle":"2025-01-09T10:54:39.773451Z","shell.execute_reply.started":"2025-01-09T10:54:39.75847Z","shell.execute_reply":"2025-01-09T10:54:39.77235Z"}},"outputs":[],"execution_count":null}]}